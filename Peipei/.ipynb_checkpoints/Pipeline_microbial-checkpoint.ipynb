{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'metadata_full_MMPRNT_G5.LC_LUX_trunc_rar_016017_v2.txt'\n",
    "richness = 'Rarefied_diversity_MMPRNT_G5_LC_LUX_016017.txt'\n",
    "richness_type = 'full_Richness'\n",
    "site = 'LC'\n",
    "site_not_used = 'LUX'\n",
    "cv_num = 10\n",
    "test_size = 0.1 ### what the proportion of your data you want to hold out as test set, \n",
    "                ### which will never be seen when you train the model\n",
    "save_model_name = 'Richness_model_build_on_LC.sav'\n",
    "save_result_name = 'Result_of_Richness_model_build_on_LC.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file, sep='\\t', index_col = 0, header = 0)\n",
    "rich = pd.read_csv(richness, sep='\\t', index_col = 0, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop([\"collectionDate\",\"siteID\",\"UTM_Lat_Cord\",\"UTM_Lon_Cord\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert FeatStatus to features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "FertStatus_1hot = cat_encoder.fit_transform(df2[[\"FertStatus\"]])\n",
    "df2.FertStatus = FertStatus_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_matrix = pd.concat([rich[richness_type],df2], axis=1, sort = False)\n",
    "ML_matrix = ML_matrix[ML_matrix[richness_type].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(ML_matrix[richness_type]))  ### If False, indicates no NaN in the value to be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Split the data to target site and the other site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df.loc[ML_matrix.index.tolist()]\n",
    "sampleID_target_site = df_3[df_3.siteID == site].index\n",
    "sampleID_test_site = df_3[df_3.siteID != site].index\n",
    "ML_matrix_target_site = ML_matrix.loc[sampleID_target_site,:]\n",
    "ML_matrix_test_site = ML_matrix.loc[sampleID_test_site,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data in target site to training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(ML_matrix_target_site, \\\n",
    "                                       test_size=test_size, random_state=42)\n",
    "X_train = train_set.drop(richness_type, axis=1) \n",
    "X_test = test_set.drop(richness_type, axis=1)\n",
    "X_on_test_site = ML_matrix_test_site.drop(richness_type, axis=1)\n",
    "\n",
    "y_train = train_set[richness_type]\n",
    "y_test = test_set[richness_type]\n",
    "y_on_test_site = ML_matrix_test_site[richness_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing data using KNN, five Ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. drop features with >50% missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Miss_count = X_train.count(0)\n",
    "Col_to_drop = Miss_count[Miss_count <= 0.5*X_train.shape[0]].index.tolist()\n",
    "Col_to_drop\n",
    "X_train.drop(Col_to_drop,axis=1,inplace=True)\n",
    "X_test.drop(Col_to_drop,axis=1,inplace=True)\n",
    "X_on_test_site.drop(Col_to_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "class KNNImputer_Ks(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, *Ks):\n",
    "        self.Ks = Ks\n",
    "    def fit(self, X,Ks):\n",
    "        D_imputer = {}        \n",
    "        for k in [3,4,5,6,7]:\n",
    "            imputer = KNNImputer(n_neighbors=k)\n",
    "            D_imputer[k] = imputer.fit(X)              \n",
    "        return D_imputer\n",
    "    def transform(self, X):\n",
    "        Impute_train = {}\n",
    "        for k in [3,4,5,6,7]:\n",
    "            Impute_train[k] = pd.DataFrame(D_imputer[k].transform(X))\n",
    "            Impute_train[k].index = X.index\n",
    "            Impute_train[k].columns = X.columns \n",
    "            if k == 3:\n",
    "                Imputed = Impute_train[k].copy(deep=True)\n",
    "                Imputed.loc[:,:] = 0\n",
    "            Imputed = Imputed.add(Impute_train[k],fill_value=0)\n",
    "        return Imputed/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_knn = KNNImputer_Ks()\n",
    "D_imputer = imputer_knn.fit(X_train, Ks=\"3,4,5,6,7\")\n",
    "X_train_KNN = imputer_knn.transform(X_train)\n",
    "X_test_KNN = imputer_knn.transform(X_test)\n",
    "X_on_test_site_KNN =  imputer_knn.transform(X_on_test_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.1s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.0s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.0s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.0s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.0s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.0s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.0s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.0s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=10 ..................\n",
      "[CV] ... max_depth=3, max_features=0.1, n_estimators=10, total=   0.0s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=100 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=500 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=500, total=   0.8s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=500 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=500, total=   0.8s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=500 .................\n",
      "[CV] .. max_depth=3, max_features=0.1, n_estimators=500, total=   0.8s\n",
      "[CV] max_depth=3, max_features=0.1, n_estimators=500 .................\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "param_grid = {'max_depth':[3, 5, 10], \\\n",
    "              'max_features': [0.1, 0.5, 'sqrt', 'log2', None], \\\n",
    "              'n_estimators': [10, 100,500,1000]}\n",
    "Reg_Mol = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(Reg_Mol, param_grid, cv=cv_num, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train_KNN, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. cross-validation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "parameter2use = grid_search.best_params_\n",
    "Reg = RandomForestRegressor(n_estimators=parameter2use['n_estimators'],\\\n",
    "                            max_depth=parameter2use['max_depth'],\\\n",
    "                            max_features= parameter2use['max_features'],\\\n",
    "                            criterion='mse', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673744377181742"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_pred = cross_val_predict(estimator=Reg, X=X_train_KNN, y=y_train, cv=cv_num)\n",
    "cv_mse = mean_squared_error(y_train, cv_pred)\n",
    "cv_evs = explained_variance_score(y_train, cv_pred)\n",
    "cv_r2 = r2_score(y_train, cv_pred)\n",
    "cv_cor = np.corrcoef(np.array(y_train), cv_pred)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg.fit(X_train_KNN,y_train)\n",
    "pred_train = Reg.predict(X_train_KNN)\n",
    "train_mse = mean_squared_error(y_train, pred_train)\n",
    "train_evs = explained_variance_score(y_train, pred_train)\n",
    "train_r2 = r2_score(y_train, pred_train)\n",
    "train_cor = np.corrcoef(np.array(y_train), pred_train)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = Reg.predict(X_test_KNN)\n",
    "test_mse = mean_squared_error(y_test, pred_test)\n",
    "test_evs = explained_variance_score(y_test, pred_test)\n",
    "test_r2 = r2_score(y_test, pred_test)\n",
    "test_cor = np.corrcoef(np.array(y_test), pred_test)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_on_test_site = Reg.predict(X_on_test_site_KNN)\n",
    "test_site_mse = mean_squared_error(y_on_test_site, pred_on_test_site)\n",
    "test_site_evs = explained_variance_score(y_on_test_site, pred_on_test_site)\n",
    "test_site_r2 = r2_score(y_on_test_site, pred_on_test_site)\n",
    "test_site_cor = np.corrcoef(np.array(y_on_test_site), pred_on_test_site)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(Reg, open(save_model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = open(save_result_name,'w')\n",
    "out.write('The model is built using data from %s, and applied to %s and %s.\\n\\n'%(site,site,site_not_used))\n",
    "out.write('There are %s training instances.\\n'%X_train_KNN.shape[0])\n",
    "out.write('There are %s test instances.\\n'%X_test_KNN.shape[0])\n",
    "out.write('There are %s instances in the other site.\\n\\n'%X_on_test_site_KNN.shape[0])\n",
    "out.write('The model is built using RandomForestRegressor, with:\\n')\n",
    "out.write('\\tn_estimators: %s\\n'%parameter2use['n_estimators'])\n",
    "out.write('\\tmax_depth: %s\\n'%parameter2use['max_depth'])\n",
    "out.write('\\tmax_features: %s\\n\\n'%parameter2use['max_features'])\n",
    "out.write('There are %s feature used\\n\\n'%X_train_KNN.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.write('Prediction\\tmse\\tevs\\tr2\\tPCC\\n')\n",
    "out.write('CV\\t%s\\t%s\\t%s\\t%s\\n'%(cv_mse,cv_evs,cv_r2,cv_cor))\n",
    "out.write('Train\\t%s\\t%s\\t%s\\t%s\\n'%(train_mse,train_evs,train_r2,train_cor))\n",
    "out.write('Test\\t%s\\t%s\\t%s\\t%s\\n'%(test_mse,test_evs,test_r2,test_cor))\n",
    "out.write('Other_site\\t%s\\t%s\\t%s\\t%s\\n\\n'%(test_site_mse,test_site_evs,test_site_r2,test_site_cor))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame({'Feature':X_train_KNN.columns, 'Importance':Reg.feature_importances_})\n",
    "imp_sorted = imp.sort_values(by='Importance', ascending=False)\n",
    "imp_sorted.to_csv(save_result_name.split('.txt')[0] + '_imp.txt', index=False, header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
